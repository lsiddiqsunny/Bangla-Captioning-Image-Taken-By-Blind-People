{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_train = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation = json.load(open(\"Dataset/annotations/train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'description': 'This dataset contains crowdsourced captions of images from VizWiz datasets. This file contains the train partition.', 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'name': 'Attribution 4.0 International (CC BY 4.0)'}, 'url': 'https://vizwiz.org', 'version': 'VizWiz-Captions 1.0', 'year': 2019, 'contributor': 'VizWiz-Captions Consortium', 'date_created': '2019-12-23'}\nTotal Images:  23431\n"
     ]
    }
   ],
   "source": [
    "print(train_annotation['info'])\n",
    "total_image = len(train_annotation['images'])\n",
    "print('Total Images: ',total_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load and show an image with Pillow\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(640, 480)\n",
      "Caption # 0 :  ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS THE NET WEIGHT TOO.\n",
      "Caption # 1 :  A green and white plastic condiment bottle containing Basil leaves.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  A bottle of spices in a plastic container laying on a surface.\n",
      "Caption # 4 :  some basil leaves in a container on a counter\n",
      "(640, 480)\n",
      "Caption # 0 :  A can of Coca Cola on a counter is shown for when one can use a nice, cold drink.\n",
      "Caption # 1 :  A black can of Coca Cola Zero calorie soda is on the counter near the coffee maker.\n",
      "Caption # 2 :  A kitchen counter the various items on top including a can of Coca-Cola, metal containers, and a teapot.\n",
      "Caption # 3 :  a black tin of Coca Cola placed on a black surface\n",
      "Caption # 4 :  Black counter with canisters, kettle and can of soda.\n",
      "(640, 480)\n",
      "Caption # 0 :  A can of crushed tomatoes are on a brown surface, the tomatoes read crushed tomatoes on the brand.\n",
      "Caption # 1 :  A can of crushed tomatoes sitting on a beige colored counter.\n",
      "Caption # 2 :  a can of crushed tomatoes in puree from price chopper.\n",
      "Caption # 3 :  a Price Chopper branded can of crushed tomatoes\n",
      "Caption # 4 :  Image is a can of crushed tomatoes in view.\n",
      "(640, 480)\n",
      "Caption # 0 :  A white screen with a captcha that needs to be completed.\n",
      "Caption # 1 :  A screenshot of a Captcha code on a phone screen.\n",
      "Caption # 2 :  Screenshot from a smartphone with a case insensitive security measure.\n",
      "Caption # 3 :  image shows a screenshot of a page required captcha.\n",
      "Caption # 4 :  A screenshot of Spotify page on a cell phone saying click here to start download, showing a captcha that says T36M (case sensitive).\n",
      "(640, 480)\n",
      "Caption # 0 :  A box for a garden light rests in someone's lap.\n",
      "Caption # 1 :  A box containing information about a solar garden light product\n",
      "Caption # 2 :  A garden book is sitting on a person's lap.\n",
      "Caption # 3 :  a box for a solar garden light laying on someone's lap\n",
      "Caption # 4 :  A blue and yellow box with lights for the garden inside.\n",
      "(640, 480)\n",
      "Caption # 0 :  A person uses a pair of brown hiking boots.\n",
      "Caption # 1 :  Two boot encased feet and blue jean clad legs are standing on an off-white floor.\n",
      "Caption # 2 :  Someone wearing brown work boots has taken a picture of a white floor.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  A person's feet wearing brown shoes on a white floor.\n",
      "(640, 480)\n",
      "Caption # 0 :  a jug of equate antiseptic mouthwash citrus flavor\n",
      "Caption # 1 :  Bottle of orange equate brand antiseptic mouth rinse that is citrus flavored.\n",
      "Caption # 2 :  A person is holding a container of orange mouthwash.\n",
      "Caption # 3 :  Orange mouthwash waits to be gargled from this bottle.\n",
      "Caption # 4 :  An orange and white packet of mouth rinse called equate is seen.\n",
      "(640, 480)\n",
      "Caption # 0 :  a fine life monitor cleaning kit from fine life\n",
      "Caption # 1 :  An unopened Monitor Cleaning Kit rests in someone's lap.\n",
      "Caption # 2 :  A package containing cleaning items to clean a monitor.\n",
      "Caption # 3 :  A packaged set of cleaning supplies for a device screen.\n",
      "Caption # 4 :  A cleaning kit is shown in plastic packaging on someone's legs\n",
      "(640, 480)\n",
      "Caption # 0 :  candy with pink color to eat and enjoy and text appear on the package\n",
      "Caption # 1 :  a yellow color cheese is packed in a transparent nylon\n",
      "Caption # 2 :  a sachet packaged of Farley' s Orange Slices\n",
      "Caption # 3 :  Tasty, orange, jelly candies are in this bag.\n",
      "Caption # 4 :  Plastic bag of Farley's brand orange slice candies.\n",
      "(640, 480)\n",
      "Caption # 0 :  A silver cover drain in the middle of a white floor.\n",
      "Caption # 1 :  A shower floor has a chrome center for water to leave.\n",
      "Caption # 2 :  A sink drain is in the large white sink\n",
      "Caption # 3 :  A chrome shower drain with the shadow of the hands of the person taking the picture\n",
      "Caption # 4 :  A metal drain cover covering a drain located in a shallow sink.\n"
     ]
    }
   ],
   "source": [
    "descriptions = dict()\n",
    "for i in range(num_to_train):\n",
    "    image = Image.open('Dataset/train/'+train_annotation['images'][i]['file_name'])\n",
    "    image = image.convert(mode='L')\n",
    "    image = image.resize((640, 480))\n",
    "    print(image.size)\n",
    "    descriptions[i] = []\n",
    "    for j in range(5):\n",
    "        print('Caption #',j,': ',train_annotation['annotations'][i*5+j]['caption'])\n",
    "        descriptions[i].append(train_annotation['annotations'][i*5+j]['caption'])\n",
    "#     pyplot.subplot(5,2,i+1)\n",
    "#     pyplot.imshow(image)\n",
    "# pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(annotation,isTrain = True):\n",
    "\t# load the model\n",
    "\tmodel = VGG16()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t# extract features from each photo\n",
    "\tfeatures = dict()\n",
    "\n",
    "\tfor i in range(num_to_train):\n",
    "\t\tif isTrain==False:\n",
    "\t\t\tif i==num_to_train//2:\n",
    "\t\t\t\tbreak\n",
    "\t\t# load an image from file\n",
    "\t\tfilename = ''\n",
    "\t\tif isTrain:\n",
    "\t\t\tfilename = 'Dataset/train/'+annotation['images'][i]['file_name']\n",
    "\t\telse:\n",
    "\t\t\tfilename = 'Dataset/val/'+annotation['images'][i]['file_name']\n",
    "\t\timage = load_img(filename, target_size=(224, 224))\n",
    "\t\t# convert the image pixels to a numpy array\n",
    "\t\timage = img_to_array(image)\n",
    "\t\t# reshape data for the model\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\t# prepare the image for the VGG model\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\t# get features\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\t# store feature\n",
    "\t\tfeatures[i] = feature\n",
    "\t\t# print('>%s' % name)\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted Features: 10\n"
     ]
    }
   ],
   "source": [
    "features = extract_features(train_annotation)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded: 10 \n"
     ]
    }
   ],
   "source": [
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\t# tokenize\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\t# convert to lower case\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\t# remove hanging 's' and 'a'\n",
    "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
    "\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary Size: 213\n"
     ]
    }
   ],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "\t# build a list of all description strings\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\tlines.append(str(key) + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# save descriptions\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Photos: train=10\n"
     ]
    }
   ],
   "source": [
    "# load photo features\n",
    "def load_photo_features(filename, num_to_train):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in range(num_to_train)}\n",
    "\treturn features\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', num_to_train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Descriptions: train=10\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, num_to_train):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = int(tokens[0]), tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in range(num_to_train):\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', num_to_train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary Size: 216\n"
     ]
    }
   ],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def max_lengths(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    " \n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t#print(desc_list)\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list.keys():\n",
    "\t\t# print(desc)\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences(desc_list[desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Description Length: 21\n"
     ]
    }
   ],
   "source": [
    "# determine the maximum sequence length\n",
    "max_length = max_lengths(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'description': 'This dataset contains crowdsourced captions of images from VizWiz datasets. This file contains the val partition.', 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'name': 'Attribution 4.0 International (CC BY 4.0)'}, 'url': 'https://vizwiz.org', 'version': 'VizWiz-Captions 1.0', 'year': 2019, 'contributor': 'VizWiz-Captions Consortium', 'date_created': '2019-12-23'}\n",
      "(640, 480)\n",
      "Caption # 0 :  A computer screen shows a repair prompt on the screen.\n",
      "Caption # 1 :  a computer screen with a repair automatically pop up\n",
      "Caption # 2 :  partial computer screen showing the need of repairs\n",
      "Caption # 3 :  Part of a computer monitor showing a computer repair message.\n",
      "Caption # 4 :  The top of a laptop with a blue background and dark blue text.\n",
      "(640, 480)\n",
      "Caption # 0 :  A person is holding a bottle that has medicine for the night time.\n",
      "Caption # 1 :  A bottle of medication has a white twist top.\n",
      "Caption # 2 :  night time medication bottle being held by someone\n",
      "Caption # 3 :  a person holding a small black bottle of NIGHT TIME\n",
      "Caption # 4 :  A bottle of what appears to be cough syrup held in hand.\n",
      "(640, 480)\n",
      "Caption # 0 :  a white paper showing an image of black and brown dog\n",
      "Caption # 1 :  A library book with pictures of two dogs on the cover on a wooden table.\n",
      "Caption # 2 :  A book with a black and a tan dog walking down a snowy street.\n",
      "Caption # 3 :  The book cover shows two dogs in the snow\n",
      "Caption # 4 :  A book cover title Dog Years with an image of a black and brown dog walking up the street, on the left side it has a due date sticker from a library.\n",
      "(640, 480)\n",
      "Caption # 0 :  A white box is to the left of a blue box on a wooden table.\n",
      "Caption # 1 :  A small rectangular red and white box next to a small rectangular blue box on a wooden surface.\n",
      "Caption # 2 :  two boxes of  medicine, one white and red and the other blue sitting on a table\n",
      "Caption # 3 :  Two boxes that appear to contain medication or eye drops\n",
      "Caption # 4 :  Two boxes of pharmaceutical products left in a table\n",
      "(640, 480)\n",
      "Caption # 0 :  close up of a computer monitor that is powered on.\n",
      "Caption # 1 :  A monitor has a message displayed on it.\n",
      "Caption # 2 :  Pictured here is a screenshot that shows an error message from an app.\n",
      "Caption # 3 :  Computer screen displaying an error saying the display driver is not supported by Zoom Text.\n",
      "Caption # 4 :  a screenshot of someone's monitor that is having issues\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted Features: 5\n"
     ]
    }
   ],
   "source": [
    "val_annotation = json.load(open(\"Dataset/annotations/val.json\"))\n",
    "print(val_annotation['info'])\n",
    "val_descriptions = dict()\n",
    "for i in range(num_to_train//2):\n",
    "    image = Image.open('Dataset/val/'+val_annotation ['images'][i]['file_name'])\n",
    "    image = image.convert(mode='L')\n",
    "    image = image.resize((640, 480))\n",
    "    print(image.size)\n",
    "    val_descriptions[i] = []\n",
    "    for j in range(5):\n",
    "        print('Caption #',j,': ',val_annotation['annotations'][i*5+j]['caption'])\n",
    "        val_descriptions[i].append(val_annotation['annotations'][i*5+j]['caption'])\n",
    "val_features = extract_features(val_annotation,False)\n",
    "print('Extracted Features: %d' % len(val_features))\n",
    "# save to file\n",
    "dump(val_features, open('val_features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary Size: 115\n",
      "Photos: val=5\n",
      "Descriptions: val=5\n",
      "Vocabulary Size: 118\n",
      "Description Length: 30\n"
     ]
    }
   ],
   "source": [
    "clean_descriptions(val_descriptions)\n",
    "# summarize vocabulary\n",
    "val_vocabulary = to_vocabulary(val_descriptions)\n",
    "print('Vocabulary Size: %d' % len(val_vocabulary))\n",
    "# save descriptions\n",
    "save_descriptions(val_descriptions, 'val_descriptions.txt')\n",
    "val_features = load_photo_features('val_features.pkl', num_to_train//2)\n",
    "print('Photos: val=%d' % len(val_features))\n",
    "val_descriptions = load_clean_descriptions('val_descriptions.txt', num_to_train//2)\n",
    "print('Descriptions: val=%d' % len(val_descriptions))\n",
    "# prepare tokenizer\n",
    "val_tokenizer = create_tokenizer(val_descriptions)\n",
    "val_vocab_size = len(val_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % val_vocab_size)\n",
    "# determine the maximum sequence length\n",
    "val_max_length = max_lengths(val_descriptions)\n",
    "print('Description Length: %d' % val_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1val, X2val, yval = create_sequences(val_tokenizer, val_max_length, val_descriptions, val_features, val_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_7\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_9 (InputLayer)            (None, 21)           0                                            \n__________________________________________________________________________________________________\ninput_8 (InputLayer)            (None, 4096)         0                                            \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 21, 256)      55296       input_9[0][0]                    \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 4096)         0           input_8[0][0]                    \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 21, 256)      0           embedding_2[0][0]                \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 256)          1048832     dropout_3[0][0]                  \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n                                                                 lstm_2[0][0]                     \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 216)          55512       dense_5[0][0]                    \n==================================================================================================\nTotal params: 1,750,744\nTrainable params: 1,750,744\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_8 to have shape (4096,) but got array with shape (1,)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-9004c9825866>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep-learning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep-learning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep-learning\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_8 to have shape (4096,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1val, X2val], yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('deep-learning': conda)",
   "language": "python",
   "name": "python37364bitdeeplearningcondaeffde72656b44ccc9f803edcd0e403e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}