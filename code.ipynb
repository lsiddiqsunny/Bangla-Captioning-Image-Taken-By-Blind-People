{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_train = 50\n",
    "num_to_val = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation = json.load(open(\"Dataset/annotations/train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'description': 'This dataset contains crowdsourced captions of images from VizWiz datasets. This file contains the train partition.', 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'name': 'Attribution 4.0 International (CC BY 4.0)'}, 'url': 'https://vizwiz.org', 'version': 'VizWiz-Captions 1.0', 'year': 2019, 'contributor': 'VizWiz-Captions Consortium', 'date_created': '2019-12-23'}\nTotal Images:  23431\n"
     ]
    }
   ],
   "source": [
    "print(train_annotation['info'])\n",
    "total_image = len(train_annotation['images'])\n",
    "print('Total Images: ',total_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load and show an image with Pillow\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(640, 480)\n",
      "Caption # 0 :  ITS IS A BASIL LEAVES CONTAINER ITS CONTAINS THE NET WEIGHT TOO.\n",
      "Caption # 1 :  A green and white plastic condiment bottle containing Basil leaves.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  A bottle of spices in a plastic container laying on a surface.\n",
      "Caption # 4 :  some basil leaves in a container on a counter\n",
      "(640, 480)\n",
      "Caption # 0 :  A can of Coca Cola on a counter is shown for when one can use a nice, cold drink.\n",
      "Caption # 1 :  A black can of Coca Cola Zero calorie soda is on the counter near the coffee maker.\n",
      "Caption # 2 :  A kitchen counter the various items on top including a can of Coca-Cola, metal containers, and a teapot.\n",
      "Caption # 3 :  a black tin of Coca Cola placed on a black surface\n",
      "Caption # 4 :  Black counter with canisters, kettle and can of soda.\n",
      "(640, 480)\n",
      "Caption # 0 :  A can of crushed tomatoes are on a brown surface, the tomatoes read crushed tomatoes on the brand.\n",
      "Caption # 1 :  A can of crushed tomatoes sitting on a beige colored counter.\n",
      "Caption # 2 :  a can of crushed tomatoes in puree from price chopper.\n",
      "Caption # 3 :  a Price Chopper branded can of crushed tomatoes\n",
      "Caption # 4 :  Image is a can of crushed tomatoes in view.\n",
      "(640, 480)\n",
      "Caption # 0 :  A white screen with a captcha that needs to be completed.\n",
      "Caption # 1 :  A screenshot of a Captcha code on a phone screen.\n",
      "Caption # 2 :  Screenshot from a smartphone with a case insensitive security measure.\n",
      "Caption # 3 :  image shows a screenshot of a page required captcha.\n",
      "Caption # 4 :  A screenshot of Spotify page on a cell phone saying click here to start download, showing a captcha that says T36M (case sensitive).\n",
      "(640, 480)\n",
      "Caption # 0 :  A box for a garden light rests in someone's lap.\n",
      "Caption # 1 :  A box containing information about a solar garden light product\n",
      "Caption # 2 :  A garden book is sitting on a person's lap.\n",
      "Caption # 3 :  a box for a solar garden light laying on someone's lap\n",
      "Caption # 4 :  A blue and yellow box with lights for the garden inside.\n",
      "(640, 480)\n",
      "Caption # 0 :  A person uses a pair of brown hiking boots.\n",
      "Caption # 1 :  Two boot encased feet and blue jean clad legs are standing on an off-white floor.\n",
      "Caption # 2 :  Someone wearing brown work boots has taken a picture of a white floor.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  A person's feet wearing brown shoes on a white floor.\n",
      "(640, 480)\n",
      "Caption # 0 :  a jug of equate antiseptic mouthwash citrus flavor\n",
      "Caption # 1 :  Bottle of orange equate brand antiseptic mouth rinse that is citrus flavored.\n",
      "Caption # 2 :  A person is holding a container of orange mouthwash.\n",
      "Caption # 3 :  Orange mouthwash waits to be gargled from this bottle.\n",
      "Caption # 4 :  An orange and white packet of mouth rinse called equate is seen.\n",
      "(640, 480)\n",
      "Caption # 0 :  a fine life monitor cleaning kit from fine life\n",
      "Caption # 1 :  An unopened Monitor Cleaning Kit rests in someone's lap.\n",
      "Caption # 2 :  A package containing cleaning items to clean a monitor.\n",
      "Caption # 3 :  A packaged set of cleaning supplies for a device screen.\n",
      "Caption # 4 :  A cleaning kit is shown in plastic packaging on someone's legs\n",
      "(640, 480)\n",
      "Caption # 0 :  candy with pink color to eat and enjoy and text appear on the package\n",
      "Caption # 1 :  a yellow color cheese is packed in a transparent nylon\n",
      "Caption # 2 :  a sachet packaged of Farley' s Orange Slices\n",
      "Caption # 3 :  Tasty, orange, jelly candies are in this bag.\n",
      "Caption # 4 :  Plastic bag of Farley's brand orange slice candies.\n",
      "(640, 480)\n",
      "Caption # 0 :  A silver cover drain in the middle of a white floor.\n",
      "Caption # 1 :  A shower floor has a chrome center for water to leave.\n",
      "Caption # 2 :  A sink drain is in the large white sink\n",
      "Caption # 3 :  A chrome shower drain with the shadow of the hands of the person taking the picture\n",
      "Caption # 4 :  A metal drain cover covering a drain located in a shallow sink.\n",
      "(640, 480)\n",
      "Caption # 0 :  A person is holding a white Samsung phone on a bed.\n",
      "Caption # 1 :  A white Samsung cell phone in a case\n",
      "Caption # 2 :  A wonderful view of the fog windows in the room is very thick\n",
      "Caption # 3 :  Caucasian hand holding a white smartphone in front of a blue striped background.\n",
      "Caption # 4 :  A person holding a white cell phone in their hand.\n",
      "(640, 480)\n",
      "Caption # 0 :  imagine how you would describe this image on the phone to a friend.\n",
      "Caption # 1 :  the photographer's hand holding a round bottle with a black lid.\n",
      "Caption # 2 :  canned food held by a man's fingers with the thumb visible\n",
      "Caption # 3 :  A person holding a food can with the rear of the label facing forward.\n",
      "Caption # 4 :  A White man's hand holding a Vitamin Bottle.\n",
      "(640, 480)\n",
      "Caption # 0 :  a jvc DVD and vcr remote someone has in their hand\n",
      "Caption # 1 :  A JVC Remote Control for a DVD and VCR.\n",
      "Caption # 2 :  A gray remote has many buttons  on it\n",
      "Caption # 3 :  PERSON HOLDING A TV REMOTE IN THEIR HAND\n",
      "Caption # 4 :  A JVC branded remote in a white hand\n",
      "(640, 480)\n",
      "Caption # 0 :  I see a can of Dr pepper on the bed\n",
      "Caption # 1 :  White can of Thor diet Dr pepper with red writing\n",
      "Caption # 2 :  Hairy chest with white  sheet  and a can of  diet Dr pepper\n",
      "Caption # 3 :  A can of Dr pepper soda sits on a bed sheet that is covering a person\n",
      "Caption # 4 :  A very hairy stomach covered with a blanket with Dr Pepper can on top of it\n",
      "(640, 480)\n",
      "Caption # 0 :  The backside of a cough drop package with its nutrition site showing\n",
      "Caption # 1 :  the back of a package of halls brand cough drops\n",
      "Caption # 2 :  A package of throat lozenges is laying on a table.\n",
      "Caption # 3 :  A big bag of halls cough suppressing mints.\n",
      "Caption # 4 :  Halls advanced vapor cough drops back panel with directions showing partially.\n",
      "(640, 480)\n",
      "Caption # 0 :  someone holding a white fabric of some sort\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  The white t-shirt is on the person and the floor is tiled and marbled like.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  Three quarters and a nickel on a wood surface.\n",
      "Caption # 1 :  Two US quarters, a nickel, and a dime, sitting on top of a wooden surface.\n",
      "Caption # 2 :  A group of small coins are on top of the table.\n",
      "Caption # 3 :  Two quarters, a nickel, and a dime sitting on a wooden surface.\n",
      "Caption # 4 :  Three quarters and one nickel on a wooden table.\n",
      "(640, 480)\n",
      "Caption # 0 :  Blear package containing an air freshener that uses purple beads.\n",
      "Caption # 1 :  Water beads fill the jar sitting on the table.\n",
      "Caption # 2 :  A photo of a jar of purple aroma fragrance spheres on a granite counter top.\n",
      "Caption # 3 :  The container is short and squatty and has a white pearl like round top with purple beads.\n",
      "Caption # 4 :  an air freshener with clear balls inside of it\n",
      "(640, 480)\n",
      "Caption # 0 :  An office room has a TV and a table with papers and a bottle of liquid on it.\n",
      "Caption # 1 :  An office or classroom with a TV on a metal AV stand is in the background of a white table with red chair and glass beverage bottle.\n",
      "Caption # 2 :  A frosted white and orange glass bottle standing on a table with a glass and bowl.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  A flat screen TV is sitting on a utility cart near a table.\n",
      "(640, 480)\n",
      "Caption # 0 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  A copy of Grand theft auto vice city for PlayStation 2.\n",
      "Caption # 1 :  Most of grand theft auto vice, a video game, is shown with the cover of the video game showing a motorcycle in the upper right corner, and an African American man with a gun in the lower right corner, and a few cars on the cover.\n",
      "Caption # 2 :  a PlayStation 2 CD pack of Grand Theft Auto\n",
      "Caption # 3 :  A case of the game grand theft auto vice city on PlayStation 2 is visible.\n",
      "Caption # 4 :  The front of a copy of the Grand Theft Auto Vice City video game.\n",
      "(640, 480)\n",
      "Caption # 0 :  Only graphs are visible and nothing more else\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  A can of kidney beans on a kitchen counter.\n",
      "Caption # 1 :  A can of Great Value light red kidney beans\n",
      "Caption # 2 :  A product tin lead on a white table.\n",
      "Caption # 3 :  a white can of canned light red kidney beans\n",
      "Caption # 4 :  A great value can of light red kidney beans is on top of the countertop next to the stove.\n",
      "(640, 480)\n",
      "Caption # 0 :  A movie container storing the CD for the movie Bridesmaids.\n",
      "Caption # 1 :  Movie called Bridesmaids From the producer of Superbad and Knocked up.\n",
      "Caption # 2 :  A cover from the movie Bridesmaids which shows four actresses in pink outfits and one actress in a wedding dress.\n",
      "Caption # 3 :  A DVD of Bridesmaids shows five women on the cover.\n",
      "Caption # 4 :  Bridesmaids DVD cover on top of a tan carpet.\n",
      "(640, 480)\n",
      "Caption # 0 :  A person holding up a white piece of paper.\n",
      "Caption # 1 :  A hand holding a paper over a black object.\n",
      "Caption # 2 :  A person's hand holding a white piece of paper.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Pictured is a portion of someone's hand holding a white piece of paper.\n",
      "(640, 480)\n",
      "Caption # 0 :  A fluffy grey poodle dog is wearing a shiny metal collar.\n",
      "Caption # 1 :  -There is a black curly haired dog\n",
      "-The dog has a silver chain collar\n",
      "-This picture shows the head and shoulders of the dog.\n",
      "Caption # 2 :  close up of a grey poodle with a chain collar.\n",
      "Caption # 3 :  grey short haired dog from behind with choker leash\n",
      "Caption # 4 :  A gray dog looking forward with a silver collar.\n",
      "(640, 480)\n",
      "Caption # 0 :  The photo  is of a bottle of  some sort of sleep  liquid, It is too blurry to make out what the product is.\n",
      "Caption # 1 :  A purple bottle with the word \"sleep\" on the front.\n",
      "Caption # 2 :  blue and sparkly bottle with the word sleep written on it\n",
      "Caption # 3 :  Front label of an small plastic bottle containing a health and beauty product.\n",
      "Caption # 4 :  A small blue bottle with little sparkles on it and the word \"sleep\".\n",
      "(640, 480)\n",
      "Caption # 0 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 1 :  A black box with nothing else in the photo.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  Possibly can being held up to camera above kitchen counter\n",
      "Caption # 1 :  Beautiful view from behind the walls hidden under dark mist\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  Close up of blue can with partial image of barcode.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  Brown and yellow piece of paper on a stove top.\n",
      "Caption # 1 :  A brown paper with Raiz El evento on the cover.\n",
      "Caption # 2 :  A brown pamphlet is on a white stove.\n",
      "Caption # 3 :  A brown flyer with a title and it is on top of a white stove.\n",
      "Caption # 4 :  A piece of paper with foreign writing on it sitting on a stove top.\n",
      "(640, 480)\n",
      "Caption # 0 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  A certificate showing a flaming torch insignia, with the text \"TOAD Training\"\n",
      "Caption # 1 :  A certificate of Toad Training from a university that is not able to be read.\n",
      "Caption # 2 :  A piece of paper with a torch on it and a name tag.\n",
      "Caption # 3 :  A sheet of paper that has a logo on it and the words Toad Training.\n",
      "Caption # 4 :  A flyer for Toad Training sponsored by a university\n",
      "(640, 480)\n",
      "Caption # 0 :  POV of a person looking at a living room with a coffee table, TV, and dining table with chairs\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  a boot on a table with a purple cup and tupperware bowl, chairs, a clock and a TV\n",
      "Caption # 3 :  A person's left foot on a wooden table with a room with a TV and a dining room in the background\n",
      "Caption # 4 :  A book sitting on a table with a purple cup and a clock on the wall that reads 4:05\n",
      "(640, 480)\n",
      "Caption # 0 :  A bottle of medicine setting on a stone countertop\n",
      "Caption # 1 :  A plastic bottle of pills that are for pain.\n",
      "Caption # 2 :  A bottle of medication in a turquoise container is sitting on a bathroom counter.\n",
      "Caption # 3 :  a small bottle of medicines are on top of a white counter\n",
      "Caption # 4 :  A bottle of pain medication is on a counter top.\n",
      "(640, 480)\n",
      "Caption # 0 :  the top of a bag of brand new socks\n",
      "Caption # 1 :  A clear bag of socks is sealed with a Ziploc.\n",
      "Caption # 2 :  Reusable clear plastic package of white men's socks.\n",
      "Caption # 3 :  Resealable plastic packaging, unable to see what the actual product is\n",
      "Caption # 4 :  The socks are packaged in a clear plastic resealable bag with green and blue parts at the top with white lettering.\n",
      "(640, 480)\n",
      "Caption # 0 :  A computer screen has a blue background and has open on it.\n",
      "Caption # 1 :  A blue screen is present on a monitor.\n",
      "Caption # 2 :  A cell phone has a bright blue screen displayed.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  an image that is blue in color and looks like a screen.\n",
      "(640, 480)\n",
      "Caption # 0 :  IMAGE IS TOO BLUR HARD TO UNDERSTAND THE IMAGE\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 4 :  Quality issues are too severe to recognize visual content.\n",
      "(640, 480)\n",
      "Caption # 0 :  A wireless keyboard for a mac computer next to a bowers & Wilkins speaker.\n",
      "Caption # 1 :  A speaker, keyboard and computer sit on a desktop\n",
      "Caption # 2 :  The electronic device says Bowers Wilkins on it and in the distance is a large computer monitor.\n",
      "Caption # 3 :  a computer set up  and a bag on a desk.\n",
      "Caption # 4 :  A computer keyboard and monitor is shown with 2 black and silver Bowers & Wilkins speakers, and a black, red, and white zippered case in the background.\n",
      "(640, 480)\n",
      "Caption # 0 :  The corner of a black display screen sitting on top of a wooden surface.\n",
      "Caption # 1 :  A black tablet is resting on a wood table top.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  The corner of a black device sitting on a wooden table.\n",
      "Caption # 4 :  A peace of glass keep on the table shown in the image.\n",
      "(640, 480)\n",
      "Caption # 0 :  Italian cuisine style dressing and seasoning mix packet\n",
      "Caption # 1 :  a sachet of Hidden Valley, FARMHOUSE ORIGINALS, HOMESTYLE ITALIAN\n",
      "Caption # 2 :  A packet of dry Hidden Valley Italian salad dressing.\n",
      "Caption # 3 :  TABLE ON PACKED SNACK ITEM FOR EAT  FOR ENTERTAINMENT\n",
      "Caption # 4 :  A package of Hidden Valley Italian dressing and seasoning mix.\n",
      "(640, 480)\n",
      "Caption # 0 :  Bar code that appears to be off of a piece of mail or package.\n",
      "Caption # 1 :  barcode on a shipping label with blue ink star\n",
      "Caption # 2 :  An airmail envelope with a bar code on it.\n",
      "Caption # 3 :  The blue mark is on the label by the bar code.\n",
      "Caption # 4 :  Partial image of an air bill used for shipping\n",
      "(640, 480)\n",
      "Caption # 0 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 1 :  Jack Link's Beef Jerky with a desiccant packet showing through the window.\n",
      "Caption # 2 :  A small bag of Jack Link's beef jerky\n",
      "Caption # 3 :  A black package of jack links beef jerky.\n",
      "Caption # 4 :  Front of a package of some kind of beef jerky.\n",
      "(640, 480)\n",
      "Caption # 0 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 1 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  White wood door on the left which is becoming more blurry to the right\n",
      "Caption # 4 :  A white sheet with folds and creases in the material\n",
      "(640, 480)\n",
      "Caption # 0 :  a container of apple pie spice containing ginger cinnamon and nutmeg\n",
      "Caption # 1 :  Label on container for natural apple pie spice\n",
      "Caption # 2 :  A plastic container that contains apple pie spice with a white and yellow label.\n",
      "Caption # 3 :  A jar of \"Apple Pie Spice\" comprised of a zesty blend of ginger, cinnamon, and nutmeg.\n",
      "Caption # 4 :  A container / package that contains various goods / edible / liquid items.\n",
      "(640, 480)\n",
      "Caption # 0 :  Heinz rice and chicken frozen TV dinner  In a purple box\n",
      "Caption # 1 :  Beautiful view from behind the walls hidden under dark mist\n",
      "Caption # 2 :  a box of Weight Watchers brand sweet and sour chicken\n",
      "Caption # 3 :  A Weight Watchers boxed dinner of Sweet & Sour chicken on a counter.\n",
      "Caption # 4 :  A ready to microwave meal of Sweet and sour chicken.\n",
      "(640, 480)\n",
      "Caption # 0 :  A pretty blue color with lighter blue on the left side.\n",
      "Caption # 1 :  A cloudless blue sky, as seen looking up.\n",
      "Caption # 2 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 3 :  A blue screen, not sure if it is a sky or a piece of blue paper.\n",
      "Caption # 4 :  blue sky without any clouds or anything in the sky at all completely clear\n",
      "(640, 480)\n",
      "Caption # 0 :  Quality issues are too severe to recognize visual content.\n",
      "Caption # 1 :  A hand holding a food container showing the nutrition label\n",
      "Caption # 2 :  hand holds a can of food with label ingredients facing forward\n",
      "Caption # 3 :  Someone is holding a can of food over a light colored linoleum floor\n",
      "Caption # 4 :  The nutrition label of a canned food item, unclear what it is\n",
      "(640, 480)\n",
      "Caption # 0 :  The back of a DVD case for the movie Hidden Figures.\n",
      "Caption # 1 :  A DVD box of the movie 'The Help', showing the back side of the box.\n",
      "Caption # 2 :  DVD case for the motion picture film The Help.\n",
      "Caption # 3 :  a yellow and black movie description most likely a DVD case with blue jeans in the background\n",
      "Caption # 4 :  A DVD case sitting on someone's lap who is wearing jeans.\n",
      "(640, 480)\n",
      "Caption # 0 :  a Hulk toy action figure on a carpet area\n",
      "Caption # 1 :  Green action figure of the Incredible Hulk lying on a brown carpet\n",
      "Caption # 2 :  Green plastic hulk toy that has purple pants on a carpeted floor\n",
      "Caption # 3 :  A toy of the Hulk wearing purple pants is on the carpet.\n",
      "Caption # 4 :  An incredible hulk action figure laying on brown carpet\n",
      "(640, 480)\n",
      "Caption # 0 :  A Kirkland water bottle is sitting on a counter.\n",
      "Caption # 1 :  A bottle of water sitting on a desk with a sheet of paper and a multi outlet power cord behind it.\n",
      "Caption # 2 :  Pictured is a bottle of water on a beige work desk.\n",
      "Caption # 3 :  A bottle of Kirkland water sits on a desk with a power strip and apple charger alongside it along with a white sheet of paper\n",
      "Caption # 4 :  A 16 ounce Kirkland brand bottle of purified water\n"
     ]
    }
   ],
   "source": [
    "descriptions = dict()\n",
    "for i in range(num_to_train):\n",
    "    image = Image.open('Dataset/train/'+train_annotation['images'][i]['file_name'])\n",
    "    image = image.convert(mode='L')\n",
    "    image = image.resize((640, 480))\n",
    "    print(image.size)\n",
    "    descriptions[i] = []\n",
    "    for j in range(5):\n",
    "        print('Caption #',j,': ',train_annotation['annotations'][i*5+j]['caption'])\n",
    "        current_caption = train_annotation['annotations'][i*5+j]['caption']\n",
    "        if current_caption == \"quality issues are too severe to recognize visual content\":\n",
    "            continue\n",
    "        \n",
    "        descriptions[i].append(current_caption)\n",
    "#     pyplot.subplot(5,2,i+1)\n",
    "#     pyplot.imshow(image)\n",
    "# pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(annotation,isTrain = True):\n",
    "\t# load the model\n",
    "\tmodel = VGG16()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t# extract features from each photo\n",
    "\tfeatures = dict()\n",
    "\n",
    "\tfor i in range(num_to_train):\n",
    "\t\tif isTrain==False:\n",
    "\t\t\tif i==num_to_val:\n",
    "\t\t\t\tbreak\n",
    "\t\t# load an image from file\n",
    "\t\tfilename = ''\n",
    "\t\tif isTrain:\n",
    "\t\t\tfilename = 'Dataset/train/'+annotation['images'][i]['file_name']\n",
    "\t\telse:\n",
    "\t\t\tfilename = 'Dataset/val/'+annotation['images'][i]['file_name']\n",
    "\t\timage = load_img(filename, target_size=(224, 224))\n",
    "\t\t# convert the image pixels to a numpy array\n",
    "\t\timage = img_to_array(image)\n",
    "\t\t# reshape data for the model\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\t# prepare the image for the VGG model\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\t# get features\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\t# store feature\n",
    "\t\tfeatures[i] = feature\n",
    "\t\t# print('>%s' % name)\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted Features: 50\n"
     ]
    }
   ],
   "source": [
    "features = extract_features(train_annotation)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded: 50 \n"
     ]
    }
   ],
   "source": [
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\t# tokenize\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\t# convert to lower case\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\t# remove hanging 's' and 'a'\n",
    "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
    "\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary Size: 631\n"
     ]
    }
   ],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "\t# build a list of all description strings\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\tlines.append(str(key) + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# save descriptions\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Photos: train=50\n"
     ]
    }
   ],
   "source": [
    "# load photo features\n",
    "def load_photo_features(filename, num_to_train):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in range(num_to_train)}\n",
    "\treturn features\n",
    "# photo features\n",
    "train_features = load_photo_features('colab/features.pkl', num_to_train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Descriptions: train=50\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, num_to_train):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = int(tokens[0]), tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in range(num_to_train):\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', num_to_train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary Size: 634\n"
     ]
    }
   ],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def max_lengths(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    " \n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t#print(desc_list)\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# print(desc)\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Description Length: 44\n"
     ]
    }
   ],
   "source": [
    "# determine the maximum sequence length\n",
    "max_length = max_lengths(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'description': 'This dataset contains crowdsourced captions of images from VizWiz datasets. This file contains the val partition.', 'license': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'name': 'Attribution 4.0 International (CC BY 4.0)'}, 'url': 'https://vizwiz.org', 'version': 'VizWiz-Captions 1.0', 'year': 2019, 'contributor': 'VizWiz-Captions Consortium', 'date_created': '2019-12-23'}\n",
      "(640, 480)\n",
      "Caption # 0 :  A computer screen shows a repair prompt on the screen.\n",
      "Caption # 1 :  a computer screen with a repair automatically pop up\n",
      "Caption # 2 :  partial computer screen showing the need of repairs\n",
      "Caption # 3 :  Part of a computer monitor showing a computer repair message.\n",
      "Caption # 4 :  The top of a laptop with a blue background and dark blue text.\n",
      "(640, 480)\n",
      "Caption # 0 :  A person is holding a bottle that has medicine for the night time.\n",
      "Caption # 1 :  A bottle of medication has a white twist top.\n",
      "Caption # 2 :  night time medication bottle being held by someone\n",
      "Caption # 3 :  a person holding a small black bottle of NIGHT TIME\n",
      "Caption # 4 :  A bottle of what appears to be cough syrup held in hand.\n",
      "(640, 480)\n",
      "Caption # 0 :  a white paper showing an image of black and brown dog\n",
      "Caption # 1 :  A library book with pictures of two dogs on the cover on a wooden table.\n",
      "Caption # 2 :  A book with a black and a tan dog walking down a snowy street.\n",
      "Caption # 3 :  The book cover shows two dogs in the snow\n",
      "Caption # 4 :  A book cover title Dog Years with an image of a black and brown dog walking up the street, on the left side it has a due date sticker from a library.\n",
      "(640, 480)\n",
      "Caption # 0 :  A white box is to the left of a blue box on a wooden table.\n",
      "Caption # 1 :  A small rectangular red and white box next to a small rectangular blue box on a wooden surface.\n",
      "Caption # 2 :  two boxes of  medicine, one white and red and the other blue sitting on a table\n",
      "Caption # 3 :  Two boxes that appear to contain medication or eye drops\n",
      "Caption # 4 :  Two boxes of pharmaceutical products left in a table\n",
      "(640, 480)\n",
      "Caption # 0 :  close up of a computer monitor that is powered on.\n",
      "Caption # 1 :  A monitor has a message displayed on it.\n",
      "Caption # 2 :  Pictured here is a screenshot that shows an error message from an app.\n",
      "Caption # 3 :  Computer screen displaying an error saying the display driver is not supported by Zoom Text.\n",
      "Caption # 4 :  a screenshot of someone's monitor that is having issues\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted Features: 5\n"
     ]
    }
   ],
   "source": [
    "val_annotation = json.load(open(\"Dataset/annotations/val.json\"))\n",
    "print(val_annotation['info'])\n",
    "val_descriptions = dict()\n",
    "for i in range(num_to_val):\n",
    "    image = Image.open('Dataset/val/'+val_annotation ['images'][i]['file_name'])\n",
    "    image = image.convert(mode='L')\n",
    "    image = image.resize((640, 480))\n",
    "    print(image.size)\n",
    "    val_descriptions[i] = []\n",
    "    for j in range(5):\n",
    "        val_caption = val_annotation['annotations'][i*5+j]['caption']\n",
    "        print('Caption #',j,': ',val_caption)\n",
    "        \n",
    "        if val_caption == \"quality issues are too severe to recognize visual content\":\n",
    "            continue\n",
    "        val_descriptions[i].append(val_caption)\n",
    "val_features = extract_features(val_annotation,False)\n",
    "print('Extracted Features: %d' % len(val_features))\n",
    "# save to file\n",
    "dump(val_features, open('val_features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary Size: 115\n"
     ]
    }
   ],
   "source": [
    "clean_descriptions(val_descriptions)\n",
    "# summarize vocabulary\n",
    "val_vocabulary = to_vocabulary(val_descriptions)\n",
    "print('Vocabulary Size: %d' % len(val_vocabulary))\n",
    "# save descriptions\n",
    "save_descriptions(val_descriptions, 'val_descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Photos: val=5\nDescriptions: val=5\nVocabulary Size: 118\nDescription Length: 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_features = load_photo_features('colab/val_features.pkl', num_to_val)\n",
    "print('Photos: val=%d' % len(val_features))\n",
    "val_descriptions = load_clean_descriptions('colab/val_descriptions.txt', num_to_val)\n",
    "print('Descriptions: val=%d' % len(val_descriptions))\n",
    "# prepare tokenizer\n",
    "val_tokenizer = create_tokenizer(val_descriptions)\n",
    "val_vocab_size = len(val_tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % val_vocab_size)\n",
    "# determine the maximum sequence length\n",
    "val_max_length = max_lengths(val_descriptions)\n",
    "print('Description Length: %d' % val_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 44, 256)      162304      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 44, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 634)          162938      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,965,178\n",
      "Trainable params: 1,965,178\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 29s 571ms/step - loss: 5.8507\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 490ms/step - loss: 5.1396\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 481ms/step - loss: 4.6986\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 4.2335\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 23s 457ms/step - loss: 3.9936\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 422ms/step - loss: 3.7105\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 22s 446ms/step - loss: 3.4622\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 23s 452ms/step - loss: 3.2201\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 22s 438ms/step - loss: 3.0650\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 22s 449ms/step - loss: 2.9578\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 23s 466ms/step - loss: 2.7772\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 2.7060\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 429ms/step - loss: 2.5926\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 395ms/step - loss: 2.5669\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 419ms/step - loss: 2.4397\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 426ms/step - loss: 2.3229\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 418ms/step - loss: 2.2339\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 415ms/step - loss: 2.1507\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 2.0662\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 414ms/step - loss: 2.0598\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 413ms/step - loss: 1.9816\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.8937\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.8001\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 21s 412ms/step - loss: 1.7803\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6955\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 25\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('Model\\model_' + str(i) + '.h5')\n",
    "\tprint(\"model_\",str(i),\" is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('deep-learning': conda)",
   "language": "python",
   "name": "python37364bitdeeplearningcondaeffde72656b44ccc9f803edcd0e403e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}